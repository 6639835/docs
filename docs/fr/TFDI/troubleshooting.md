# üõ†Ô∏è D√©pannage du convertisseur de donn√©es de navigation TFDI

## üö® Erreurs courantes et solutions

### 1. Probl√®mes d'environnement et d'installation

#### ‚ùå Probl√®mes d'environnement Python

**Message d'erreur :**
```
ModuleNotFoundError: No module named 'rich'
ImportError: No module named 'pandas'
```

**Solution :**
```bash
# 1. V√©rifier la version de Python
python --version  # S'assurer que la version est ‚â• 3.8

# 2. Mettre √† jour pip
python -m pip install --upgrade pip

# 3. Installer les d√©pendances
pip install -r requirements.txt

# 4. V√©rifier l'installation
python -c "import rich, pandas; print('D√©pendances install√©es avec succ√®s')"
```

#### ‚ùå Erreurs d'autorisation

**Message d'erreur :**
```
PermissionError: [Errno 13] Permission denied
Impossible de cr√©er le r√©pertoire de sortie
```

**Solution :**
```bash
# Windows - Ex√©cuter en tant qu'administrateur
# Clic droit sur l'invite de commande ‚Üí "Ex√©cuter en tant qu'administrateur"

# macOS/Linux - Utiliser sudo ou modifier les permissions
sudo python converter.py
# ou
chmod 755 /path/to/output/directory
```

### 2. Probl√®mes d'acc√®s √† la base de donn√©es

#### ‚ùå Fichier de base de donn√©es introuvable

**Message d'erreur :**
```
FileNotFoundError: [Errno 2] No such file or directory: 'nd.db3'
Fichier de base de donn√©es Fenix introuvable
```

**Solution :**
1.  **V√©rifier l'installation de Fenix** :
    ```bash
    # Chemin courant
    %APPDATA%\Microsoft Flight Simulator\Packages\fenix-a320\
    ```

2.  **Localiser manuellement le fichier** :
    ```bash
    # Windows
    dir /s nd.db3
    
    # macOS/Linux
    find ~ -name "nd.db3" 2>/dev/null
    ```

3.  **R√©g√©n√©rer la base de donn√©es** :
    -   Lancer MSFS
    -   Charger le Fenix A320
    -   Attendre que les donn√©es de navigation soient charg√©es

#### ‚ùå Base de donn√©es corrompue

**Message d'erreur :**
```
sqlite3.DatabaseError: database disk image is malformed
Le fichier de base de donn√©es est corrompu
```

**M√©thode de diagnostic :**
```python
import sqlite3

def check_database_integrity(db_path):
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("PRAGMA integrity_check")
        result = cursor.fetchone()
        if result[0] == "ok":
            print("‚úÖ Int√©grit√© de la base de donn√©es normale")
        else:
            print(f"‚ùå Base de donn√©es corrompue: {result[0]}")
    except Exception as e:
        print(f"‚ùå Impossible d'acc√©der √† la base de donn√©es: {e}")
    finally:
        conn.close()
```

**Solution de r√©paration :**
```bash
# 1. Sauvegarder la base de donn√©es corrompue
cp nd.db3 nd.db3.backup

# 2. Tenter une r√©paration SQLite
sqlite3 nd.db3 ".dump" | sqlite3 nd_repaired.db3

# 3. Si la r√©paration √©choue, r√©cup√©rer la base de donn√©es
# Supprimer le fichier et red√©marrer Fenix
```

#### ‚ùå Structure de table de base de donn√©es incompatible

**Message d'erreur :**
```
sqlite3.OperationalError: no such table: Terminals
La base de donn√©es manque de tables n√©cessaires
```

**Script de validation :**
```python
def validate_database_schema(db_path):
    required_tables = [
        'Airports', 'Runways', 'Waypoints', 'Navaids',
        'Airways', 'AirwayLegs', 'Terminals', 'TerminalLegs',
        'ILSes', 'AirportLookup', 'NavaidLookup', 'WaypointLookup'
    ]
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    existing_tables = {row[0] for row in cursor.fetchall()}
    
    missing_tables = set(required_tables) - existing_tables
    if missing_tables:
        print(f"‚ùå Tables manquantes: {missing_tables}")
        return False
    
    print("‚úÖ Validation de la structure de la base de donn√©es r√©ussie")
    return True
```

### 3. Probl√®mes de m√©moire et de performance

#### ‚ùå M√©moire insuffisante

**Message d'erreur :**
```
MemoryError: Unable to allocate array
M√©moire insuffisante, impossible de traiter les donn√©es
```

**Surveillance de l'utilisation de la m√©moire :**
```python
import psutil
import gc

def monitor_memory_usage():
    memory = psutil.virtual_memory()
    print(f"M√©moire totale: {memory.total // 1024**3} GB")
    print(f"M√©moire utilis√©e: {memory.used // 1024**3} GB")
    print(f"M√©moire disponible: {memory.available // 1024**3} GB")
    print(f"Utilisation: {memory.percent}%")

def optimize_memory():
    # Forcer le nettoyage de la m√©moire (garbage collection)
    gc.collect()
    
    # Nettoyer le cache pandas
    import pandas as pd
    pd.reset_option('all')
```

**Solution :**
```python
# 1. R√©duire la taille du lot
config = ConverterConfig(
    batch_size=500,  # R√©duire de la valeur par d√©faut 1000
    coordinate_precision=6  # R√©duire la pr√©cision
)

# 2. Activer le traitement en flux
def process_large_table_streaming(table_name):
    chunk_size = 1000
    offset = 0
    
    while True:
        query = f"""
        SELECT * FROM {table_name} 
        LIMIT {chunk_size} OFFSET {offset}
        """
        
        chunk = pd.read_sql_query(query, conn)
        if chunk.empty:
            break
            
        process_chunk(chunk)
        del chunk  # Lib√©rer la m√©moire
        gc.collect()
        
        offset += chunk_size
```

#### ‚ùå Vitesse de traitement trop lente

**Sympt√¥me :** Le processus de conversion reste bloqu√© √† une certaine √©tape pendant une longue p√©riode

**Diagnostic de performance :**
```python
import time
import cProfile

def profile_conversion():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # Ex√©cuter la conversion
    converter.convert(db_path, terminal_id)
    
    profiler.disable()
    profiler.dump_stats('conversion_profile.prof')

# Analyser les goulots d'√©tranglement de performance
# python -m cProfile -o profile.prof converter.py
# python -c "import pstats; pstats.Stats('profile.prof').sort_stats('cumulative').print_stats(10)"
```

**Suggestions d'optimisation :**
```python
# 1. Optimisation des performances SQLite
def optimize_sqlite_connection(conn):
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA cache_size=10000")
    conn.execute("PRAGMA temp_store=MEMORY")

# 2. Traitement parall√®le
from concurrent.futures import ThreadPoolExecutor

def parallel_table_processing():
    tables = ['Airports', 'Runways', 'Waypoints', 'Navaids']
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for table in tables:
            future = executor.submit(process_table, table)
            futures.append(future)
        
        # Attendre que toutes les t√¢ches soient termin√©es
        for future in futures:
            future.result()
```

### 4. Probl√®mes de conversion de donn√©es

#### ‚ùå Donn√©es de coordonn√©es anormales

**Message d'erreur :**
```
ValueError: Invalid coordinate value: latitude=91.5
Coordonn√©es hors limites valides
```

**Validation et r√©paration :**
```python
def validate_and_fix_coordinates(df):
    """Valider et r√©parer les donn√©es de coordonn√©es"""
    initial_count = len(df)
    
    # V√©rifier la plage de latitude [-90, 90]
    invalid_lat = (df['Latitude'] < -90) | (df['Latitude'] > 90)
    if invalid_lat.any():
        print(f"D√©tect√© {invalid_lat.sum()} valeurs de latitude invalides")
        df = df[~invalid_lat]
    
    # V√©rifier la plage de longitude [-180, 180]
    invalid_lon = (df['Longitude'] < -180) | (df['Longitude'] > 180)
    if invalid_lon.any():
        print(f"D√©tect√© {invalid_lon.sum()} valeurs de longitude invalides")
        df = df[~invalid_lon]
    
    removed_count = initial_count - len(df)
    if removed_count > 0:
        print(f"‚ö†Ô∏è Supprim√© {removed_count} enregistrements de coordonn√©es invalides")
    
    return df
```

#### ‚ùå √âchec de la s√©rialisation JSON

**Message d'erreur :**
```
TypeError: Object of type 'datetime' is not JSON serializable
Erreur de s√©rialisation JSON
```

**Solution :**
```python
import json
from datetime import datetime
import numpy as np

class CustomJSONEncoder(json.JSONEncoder):
    """Encodeur JSON personnalis√©"""
    
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

# Utiliser l'encodeur personnalis√©
def safe_json_dump(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, cls=CustomJSONEncoder, 
                 ensure_ascii=False, indent=2)
```

#### ‚ùå Probl√®mes d'encodage de caract√®res

**Message d'erreur :**
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
Erreur d'encodage de caract√®res
```

**Solution :**
```python
import chardet

def detect_and_convert_encoding(file_path):
    """D√©tecter et convertir l'encodage du fichier"""
    # D√©tecter l'encodage
    with open(file_path, 'rb') as f:
        raw_data = f.read()
        encoding = chardet.detect(raw_data)['encoding']
    
    print(f"Encodage d√©tect√©: {encoding}")
    
    # Convertir en UTF-8
    with open(file_path, 'r', encoding=encoding) as f:
        content = f.read()
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

def safe_string_handling(text):
    """Gestion s√©curis√©e des cha√Ænes"""
    if isinstance(text, bytes):
        # Essayer plusieurs encodages
        for encoding in ['utf-8', 'gbk', 'latin1']:
            try:
                return text.decode(encoding)
            except UnicodeDecodeError:
                continue
        # Si tous √©chouent, utiliser la gestion d'erreurs
        return text.decode('utf-8', errors='replace')
    return str(text)
```

### 5. Probl√®mes de fichiers de sortie

#### ‚ùå √âchec de la cr√©ation de l'archive

**Message d'erreur :**
```
py7zr.exceptions.Bad7zFile: not a 7z file
√âchec de la cr√©ation de l'archive
```

**Solution :**
```python
import py7zr
import shutil
from pathlib import Path

def safe_create_archive(source_dir, archive_path):
    """Cr√©er une archive en toute s√©curit√©"""
    try:
        # S'assurer que le r√©pertoire source existe
        if not Path(source_dir).exists():
            raise FileNotFoundError(f"R√©pertoire source introuvable: {source_dir}")
        
        # Supprimer l'archive existante
        if Path(archive_path).exists():
            Path(archive_path).unlink()
        
        # Cr√©er l'archive
        with py7zr.SevenZipFile(archive_path, 'w') as archive:
            archive.writeall(source_dir, ".")
        
        print(f"‚úÖ Archive cr√©√©e avec succ√®s: {archive_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå √âchec de la cr√©ation de l'archive: {e}")
        
        # Solution de repli : cr√©er un fichier ZIP
        try:
            shutil.make_archive(
                str(Path(archive_path).with_suffix('')), 
                'zip', 
                source_dir
            )
            print("‚úÖ Fichier de secours au format ZIP cr√©√©")
            return True
        except Exception as zip_error:
            print(f"‚ùå La cr√©ation ZIP a √©galement √©chou√©: {zip_error}")
            return False
```

#### ‚ùå Taille de fichier anormale

**Sympt√¥me :** Le fichier de sortie est trop petit ou trop grand

**M√©thode de v√©rification :**
```python
def validate_output_files(output_dir):
    """Valider les fichiers de sortie"""
    expected_files = [
        'Airports.json', 'Runways.json', 'Waypoints.json',
        'Navaids.json', 'Airways.json', 'AirwayLegs.json',
        'Terminals.json', 'ILSes.json'
    ]
    
    file_info = {}
    for file_name in expected_files:
        file_path = Path(output_dir) / file_name
        if file_path.exists():
            size = file_path.stat().st_size
            file_info[file_name] = {
                'exists': True,
                'size_mb': size / 1024 / 1024,
                'empty': size == 0
            }
        else:
            file_info[file_name] = {'exists': False}
    
    # Imprimer les informations sur les fichiers
    print("üìÅ V√©rification des fichiers de sortie:")
    for file_name, info in file_info.items():
        if info['exists']:
            if info.get('empty', False):
                print(f"‚ö†Ô∏è {file_name}: Fichier vide")
            else:
                print(f"‚úÖ {file_name}: {info['size_mb']:.2f} MB")
        else:
            print(f"‚ùå {file_name}: Fichier manquant")
    
    return file_info
```

## üîç Outils de diagnostic

### 1. V√©rification de l'environnement syst√®me

```python
def system_diagnostics():
    """Diagnostic de l'environnement syst√®me"""
    import platform
    import sys
    import psutil
    
    print("üîç Diagnostic de l'environnement syst√®me")
    print("=" * 50)
    
    # Informations sur le syst√®me d'exploitation
    print(f"Syst√®me d'exploitation: {platform.system()} {platform.release()}")
    print(f"Architecture: {platform.machine()}")
    
    # Environnement Python
    print(f"Version Python: {sys.version}")
    print(f"Chemin Python: {sys.executable}")
    
    # Informations mat√©rielles
    print(f"Nombre de c≈ìurs CPU: {psutil.cpu_count()}")
    memory = psutil.virtual_memory()
    print(f"M√©moire totale: {memory.total // 1024**3} GB")
    print(f"M√©moire disponible: {memory.available // 1024**3} GB")
    
    # Espace disque
    disk = psutil.disk_usage('.')
    print(f"Espace disque total: {disk.total // 1024**3} GB")
    print(f"Espace disque disponible: {disk.free // 1024**3} GB")
```

### 2. Validation des paquets de d√©pendances

```python
def verify_dependencies():
    """Valider les paquets de d√©pendances"""
    required_packages = [
        'rich', 'pandas', 'py7zr', 'sqlite3'
    ]
    
    print("üì¶ Validation des paquets de d√©pendances")
    print("=" * 30)
    
    for package in required_packages:
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'Unknown')
            print(f"‚úÖ {package}: {version}")
        except ImportError:
            print(f"‚ùå {package}: Non install√©")
        except Exception as e:
            print(f"‚ö†Ô∏è {package}: Erreur - {e}")
```

### 3. Outil de surveillance des performances

```python
import time
import threading
from contextlib import contextmanager

class PerformanceMonitor:
    """Moniteur de performance"""
    
    def __init__(self):
        self.metrics = {}
        self.monitoring = False
    
    @contextmanager
    def measure(self, operation_name):
        """Mesurer le temps d'ex√©cution de l'op√©ration"""
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.virtual_memory().used
            
            self.metrics[operation_name] = {
                'duration': end_time - start_time,
                'memory_delta': end_memory - start_memory
            }
    
    def start_monitoring(self):
        """D√©marrer la surveillance en temps r√©el"""
        self.monitoring = True
        
        def monitor():
            while self.monitoring:
                cpu_percent = psutil.cpu_percent()
                memory = psutil.virtual_memory()
                
                print(f"\rüîÑ CPU: {cpu_percent:5.1f}% | "
                      f"M√©moire: {memory.percent:5.1f}% | "
                      f"Disponible: {memory.available//1024**2:,} MB", 
                      end='', flush=True)
                
                time.sleep(1)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
    
    def stop_monitoring(self):
        """Arr√™ter la surveillance"""
        self.monitoring = False
        print()  # Saut de ligne
    
    def print_summary(self):
        """Imprimer le r√©sum√© des performances"""
        print("\nüìä R√©sum√© des performances:")
        print("-" * 40)
        
        for operation, metrics in self.metrics.items():
            duration = metrics['duration']
            memory_mb = metrics['memory_delta'] / 1024 / 1024
            
            print(f"{operation:20s}: {duration:8.2f}s | "
                  f"{memory_mb:+8.1f}MB")

# Exemple d'utilisation
monitor = PerformanceMonitor()
monitor.start_monitoring()

with monitor.measure("Validation de la base de donn√©es"):
    validate_database(db_path)

with monitor.measure("Conversion des donn√©es"):
    convert_data()

monitor.stop_monitoring()
monitor.print_summary()
```

## üìã Liste de contr√¥le du d√©pannage

### üîß V√©rifications avant la conversion
- [ ] Version de Python ‚â• 3.8
- [ ] Tous les paquets de d√©pendances sont install√©s et les versions sont correctes
- [ ] Le fichier de base de donn√©es Fenix existe et est intact
- [ ] M√©moire disponible suffisante (4 Go+ recommand√©s)
- [ ] Espace disque suffisant (1 Go+ recommand√©)
- [ ] Le r√©pertoire de sortie a les permissions d'√©criture

### üìä V√©rifications pendant la conversion
- [ ] Connexion √† la base de donn√©es r√©ussie
- [ ] Toutes les tables requises existent
- [ ] Les donn√©es de coordonn√©es sont dans la plage valide
- [ ] L'utilisation de la m√©moire est dans une plage raisonnable
- [ ] Aucune erreur d'autorisation n'est apparue
- [ ] Les fichiers temporaires sont cr√©√©s normalement

### üìÅ Validation apr√®s la conversion
- [ ] Tous les fichiers JSON ont √©t√© g√©n√©r√©s
- [ ] La taille des fichiers est raisonnable (ni vide ni anormalement grande)
- [ ] Le format JSON est valide
- [ ] L'archive a √©t√© cr√©√©e avec succ√®s
- [ ] Les fichiers temporaires ont √©t√© nettoy√©s
- [ ] Le journal ne contient pas d'erreurs graves

## üÜò Obtenir de l'aide

### Auto-diagnostic
1.  **Ex√©cuter les outils de diagnostic** :
    ```python
    from tfdi_converter.diagnostics import run_full_diagnostics
    run_full_diagnostics()
    ```

2.  **Afficher les journaux d√©taill√©s** :
    ```bash
    tail -f converter.log
    grep -i error converter.log
    ```

3.  **V√©rifier les ressources syst√®me** :
    ```bash
    # Windows
    taskmgr
    
    # macOS
    activity monitor
    
    # Linux
    top
    htop
    ```

### Support communautaire
-   **GitHub Issues** : Signaler les bugs et les probl√®mes techniques
-   **GitHub Discussions** : Questions d'utilisation et partage d'exp√©rience
-   **Documentation du projet** : Consulter le guide d'utilisation complet

### Lors du signalement d'un probl√®me, veuillez fournir :
-   **Journal d'erreurs complet**
-   **Informations sur l'environnement syst√®me**
-   **Version du convertisseur**
-   **Informations sur la base de donn√©es** (taille, AIRAC, etc.)
-   **√âtapes de reproduction**
-   **Fichiers de configuration pertinents**

---

**Rencontrez-vous un probl√®me non r√©solu ?** 

Veuillez cr√©er un nouveau probl√®me sur [GitHub Issues](https://github.com/your-org/tfdi-converter/issues), nous vous aiderons √† le r√©soudre d√®s que possible ! üöÅ‚ú®
