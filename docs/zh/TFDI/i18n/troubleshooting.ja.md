# ğŸ› ï¸ TFDI ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## ğŸš¨ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–

### 1. ç’°å¢ƒã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«é–¢ã™ã‚‹å•é¡Œ

#### âŒ Python ç’°å¢ƒã®å•é¡Œ

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
ModuleNotFoundError: No module named 'rich'
ImportError: No module named 'pandas'
```

**è§£æ±ºç­–ï¼š**
```bash
# 1. Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèª
python --version  # 3.8ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª

# 2. pipã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
python -m pip install --upgrade pip

# 3. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# 4. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ç¢ºèª
python -c "import rich, pandas; print('ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«æˆåŠŸã—ã¾ã—ãŸ')"
```

#### âŒ æ¨©é™ã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
PermissionError: [Errno 13] Permission denied
å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã§ãã¾ã›ã‚“
```

**è§£æ±ºç­–ï¼š**
```bash
# Windows - ç®¡ç†è€…ã¨ã—ã¦å®Ÿè¡Œ
# ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ ã€Œç®¡ç†è€…ã¨ã—ã¦å®Ÿè¡Œã€

# macOS/Linux - sudoã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€æ¨©é™ã‚’å¤‰æ›´
sudo python converter.py
# ã¾ãŸã¯
chmod 755 /path/to/output/directory
```

### 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã«é–¢ã™ã‚‹å•é¡Œ

#### âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
FileNotFoundError: [Errno 2] No such file or directory: 'nd.db3'
Fenixãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“
```

**è§£æ±ºç­–ï¼š**
1.  **Fenixã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèª**ï¼š
    ```bash
    # ä¸€èˆ¬çš„ãªãƒ‘ã‚¹
    %APPDATA%\Microsoft Flight Simulator\Packages\fenix-a320\
    ```

2.  **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å‹•ã§ç‰¹å®š**ï¼š
    ```bash
    # Windows
    dir /s nd.db3
       
    # macOS/Linux
    find ~ -name "nd.db3" 2>/dev/null
    ```

3.  **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å†ç”Ÿæˆ**ï¼š
    -   MSFSã‚’èµ·å‹•
    -   Fenix A320ã‚’ãƒ­ãƒ¼ãƒ‰
    -   ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ

#### âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç ´æ

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
sqlite3.DatabaseError: database disk image is malformed
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã¾ã™
```

**è¨ºæ–­æ–¹æ³•ï¼š**
```python
import sqlite3

def check_database_integrity(db_path):
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("PRAGMA integrity_check")
        result = cursor.fetchone()
        if result[0] == "ok":
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ•´åˆæ€§ã¯æ­£å¸¸ã§ã™")
        else:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç ´æ: {result[0]}")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {e}")
    finally:
        conn.close()
```

**å¾©æ—§ç­–ï¼š**
```bash
# 1. ç ´æã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
cp nd.db3 nd.db3.backup

# 2. SQLiteã«ã‚ˆã‚‹ä¿®å¾©ã‚’è©¦ã¿ã‚‹
sqlite3 nd.db3 ".dump" | sqlite3 nd_repaired.db3

# 3. ä¿®å¾©ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å†å–å¾—
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã€Fenixã‚’å†èµ·å‹•
```

#### âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã«äº’æ›æ€§ãŒãªã„

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
sqlite3.OperationalError: no such table: Terminals
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“
```

**æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼š**
```python
def validate_database_schema(db_path):
    required_tables = [
        'Airports', 'Runways', 'Waypoints', 'Navaids',
        'Airways', 'AirwayLegs', 'Terminals', 'TerminalLegs',
        'ILSes', 'AirportLookup', 'NavaidLookup', 'WaypointLookup'
    ]
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    existing_tables = {row[0] for row in cursor.fetchall()}
    
    missing_tables = set(required_tables) - existing_tables
    if missing_tables:
        print(f"âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_tables}")
        return False
    
    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ ã®æ¤œè¨¼ã«æˆåŠŸã—ã¾ã—ãŸ")
    return True
```

### 3. ãƒ¡ãƒ¢ãƒªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«é–¢ã™ã‚‹å•é¡Œ

#### âŒ ãƒ¡ãƒ¢ãƒªä¸è¶³

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
MemoryError: Unable to allocate array
ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã§ãã¾ã›ã‚“
```

**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–ï¼š**
```python
import psutil
import gc

def monitor_memory_usage():
    memory = psutil.virtual_memory()
    print(f"ç·ãƒ¡ãƒ¢ãƒª: {memory.total // 1024**3} GB")
    print(f"ä½¿ç”¨æ¸ˆã¿ãƒ¡ãƒ¢ãƒª: {memory.used // 1024**3} GB")
    print(f"åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {memory.available // 1024**3} GB")
    print(f"ä½¿ç”¨ç‡: {memory.percent}%")

def optimize_memory():
    # å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    gc.collect()
    
    # pandasã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
    import pandas as pd
    pd.reset_option('all')
```

**è§£æ±ºç­–ï¼š**
```python
# 1. ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
config = ConverterConfig(
    batch_size=500,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®1000ã‹ã‚‰æ¸›ã‚‰ã™
    coordinate_precision=6  # ç²¾åº¦ã‚’ä½ä¸‹ã•ã›ã‚‹
)

# 2. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’æœ‰åŠ¹ã«ã™ã‚‹
def process_large_table_streaming(table_name):
    chunk_size = 1000
    offset = 0
    
    while True:
        query = f"""
        SELECT * FROM {table_name} 
        LIMIT {chunk_size} OFFSET {offset}
        """
        
        chunk = pd.read_sql_query(query, conn)
        if chunk.empty:
            break
            
        process_chunk(chunk)
        del chunk  # ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
        gc.collect()
        
        offset += chunk_size
```

#### âŒ å‡¦ç†é€Ÿåº¦ãŒé…ã™ãã‚‹

**ç—‡çŠ¶ï¼š** å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹ãŒç‰¹å®šã®ã‚¹ãƒ†ãƒƒãƒ—ã§é•·æ™‚é–“åœæ­¢ã™ã‚‹

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ºæ–­ï¼š**
```python
import time
import cProfile

def profile_conversion():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # å¤‰æ›ã‚’å®Ÿè¡Œ
    converter.convert(db_path, terminal_id)
    
    profiler.disable()
    profiler.dump_stats('conversion_profile.prof')

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®åˆ†æ
# python -m cProfile -o profile.prof converter.py
# python -c "import pstats; pstats.Stats('profile.prof').sort_stats('cumulative').print_stats(10)"
```

**æœ€é©åŒ–ã®æ¨å¥¨äº‹é …ï¼š**
```python
# 1. SQLiteãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–
def optimize_sqlite_connection(conn):
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA cache_size=10000")
    conn.execute("PRAGMA temp_store=MEMORY")

# 2. ä¸¦åˆ—å‡¦ç†
from concurrent.futures import ThreadPoolExecutor

def parallel_table_processing():
    tables = ['Airports', 'Runways', 'Waypoints', 'Navaids']
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for table in tables:
            future = executor.submit(process_table, table)
            futures.append(future)
        
        # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
        for future in futures:
            future.result()
```

### 4. ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã«é–¢ã™ã‚‹å•é¡Œ

#### âŒ åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
ValueError: Invalid coordinate value: latitude=91.5
åº§æ¨™ãŒæœ‰åŠ¹ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™
```

**æ¤œè¨¼ã¨ä¿®æ­£ï¼š**
```python
def validate_and_fix_coordinates(df):
    """åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼ã—ä¿®æ­£ã—ã¾ã™"""
    initial_count = len(df)
    
    # ç·¯åº¦ç¯„å›²[-90, 90]ã‚’ç¢ºèª
    invalid_lat = (df['Latitude'] < -90) | (df['Latitude'] > 90)
    if invalid_lat.any():
        print(f"ç„¡åŠ¹ãªç·¯åº¦å€¤ãŒ {invalid_lat.sum()} å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        df = df[~invalid_lat]
    
    # çµŒåº¦ç¯„å›²[-180, 180]ã‚’ç¢ºèª
    invalid_lon = (df['Longitude'] < -180) | (df['Longitude'] > 180)
    if invalid_lon.any():
        print(f"ç„¡åŠ¹ãªçµŒåº¦å€¤ãŒ {invalid_lon.sum()} å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        df = df[~invalid_lon]
    
    removed_count = initial_count - len(df)
    if removed_count > 0:
        print(f"âš ï¸ ç„¡åŠ¹ãªåº§æ¨™ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ {removed_count} å€‹å‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
    
    return df
```

#### âŒ JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã®å¤±æ•—

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
TypeError: Object of type 'datetime' is not JSON serializable
JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã‚¨ãƒ©ãƒ¼
```

**è§£æ±ºç­–ï¼š**
```python
import json
from datetime import datetime
import numpy as np

class CustomJSONEncoder(json.JSONEncoder):
    """ã‚«ã‚¹ã‚¿ãƒ JSONã‚¨ãƒ³ã‚³ãƒ¼ãƒ€"""
    
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

# ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ç”¨
def safe_json_dump(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, cls=CustomJSONEncoder, 
                 ensure_ascii=False, indent=2)
```

#### âŒ æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œ

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼
```

**è§£æ±ºç­–ï¼š**
```python
import chardet

def detect_and_convert_encoding(file_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºã—å¤‰æ›ã—ã¾ã™"""
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º
    with open(file_path, 'rb') as f:
        raw_data = f.read()
        encoding = chardet.detect(raw_data)['encoding']
    
    print(f"æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding}")
    
    # UTF-8ã«å¤‰æ›
    with open(file_path, 'r', encoding=encoding) as f:
        content = f.read()
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

def safe_string_handling(text):
    """å®‰å…¨ãªæ–‡å­—åˆ—å‡¦ç†"""
    if isinstance(text, bytes):
        # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
        for encoding in ['utf-8', 'gbk', 'latin1']:
            try:
                return text.decode(encoding)
            except UnicodeDecodeError:
                continue
        # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚’ä½¿ç”¨
        return text.decode('utf-8', errors='replace')
    return str(text)
```

### 5. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«é–¢ã™ã‚‹å•é¡Œ

#### âŒ åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã®å¤±æ•—

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**
```
py7zr.exceptions.Bad7zFile: not a 7z file
åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ
```

**è§£æ±ºç­–ï¼š**
```python
import py7zr
import shutil
from pathlib import Path

def safe_create_archive(source_dir, archive_path):
    """å®‰å…¨ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ä½œæˆã—ã¾ã™"""
    try:
        # ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        if not Path(source_dir).exists():
            raise FileNotFoundError(f"ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {source_dir}")
        
        # æ—¢å­˜ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’å‰Šé™¤
        if Path(archive_path).exists():
            Path(archive_path).unlink()
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ä½œæˆ
        with py7zr.SevenZipFile(archive_path, 'w') as archive:
            archive.writeall(source_dir, ".")
        
        print(f"âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆæˆåŠŸ: {archive_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆå¤±æ•—: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ˆï¼šZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        try:
            shutil.make_archive(
                str(Path(archive_path).with_suffix('')), 
                'zip', 
                source_dir
            )
            print("âœ… ZIPå½¢å¼ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ")
            return True
        except Exception as zip_error:
            print(f"âŒ ZIPä½œæˆã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {zip_error}")
            return False
```

#### âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç•°å¸¸

**ç—‡çŠ¶ï¼š** å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå°ã•ã™ãã‚‹ã‹å¤§ãã™ãã‚‹

**ç¢ºèªæ–¹æ³•ï¼š**
```python
def validate_output_files(output_dir):
    """å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ã—ã¾ã™"""
    expected_files = [
        'Airports.json', 'Runways.json', 'Waypoints.json',
        'Navaids.json', 'Airways.json', 'AirwayLegs.json',
        'Terminals.json', 'ILSes.json'
    ]
    
    file_info = {}
    for file_name in expected_files:
        file_path = Path(output_dir) / file_name
        if file_path.exists():
            size = file_path.stat().st_size
            file_info[file_name] = {
                'exists': True,
                'size_mb': size / 1024 / 1024,
                'empty': size == 0
            }
        else:
            file_info[file_name] = {'exists': False}
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    print("ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯:")
    for file_name, info in file_info.items():
        if info['exists']:
            if info.get('empty', False):
                print(f"âš ï¸ {file_name}: ç©ºãƒ•ã‚¡ã‚¤ãƒ«")
            else:
                print(f"âœ… {file_name}: {info['size_mb']:.2f} MB")
        else:
            print(f"âŒ {file_name}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    return file_info
```

## ğŸ” è¨ºæ–­ãƒ„ãƒ¼ãƒ«

### 1. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒãƒã‚§ãƒƒã‚¯

```python
def system_diagnostics():
    """ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨ºæ–­"""
    import platform
    import sys
    import psutil
    
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨ºæ–­")
    print("=" * 50)
    
    # ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    print(f"ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ : {platform.system()} {platform.release()}")
    print(f"ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {platform.machine()}")
    
    # Pythonç’°å¢ƒ
    print(f"Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    print(f"Python ãƒ‘ã‚¹: {sys.executable}")
    
    # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±
    print(f"CPU ã‚³ã‚¢æ•°: {psutil.cpu_count()}")
    memory = psutil.virtual_memory()
    print(f"ç·ãƒ¡ãƒ¢ãƒª: {memory.total // 1024**3} GB")
    print(f"åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {memory.available // 1024**3} GB")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
    disk = psutil.disk_usage('.')
    print(f"ãƒ‡ã‚£ã‚¹ã‚¯ç·å®¹é‡: {disk.total // 1024**3} GB")
    print(f"ãƒ‡ã‚£ã‚¹ã‚¯åˆ©ç”¨å¯èƒ½å®¹é‡: {disk.free // 1024**3} GB")
```

### 2. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æ¤œè¨¼

```python
def verify_dependencies():
    """ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æ¤œè¨¼ã—ã¾ã™"""
    required_packages = [
        'rich', 'pandas', 'py7zr', 'sqlite3'
    ]
    
    print("ğŸ“¦ ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æ¤œè¨¼")
    print("=" * 30)
    
    for package in required_packages:
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'Unknown')
            print(f"âœ… {package}: {version}")
        except ImportError:
            print(f"âŒ {package}: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        except Exception as e:
            print(f"âš ï¸ {package}: ã‚¨ãƒ©ãƒ¼ - {e}")
```

### 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ„ãƒ¼ãƒ«

```python
import time
import threading
from contextlib import contextmanager

class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.metrics = {}
        self.monitoring = False
    
    @contextmanager
    def measure(self, operation_name):
        """æ“ä½œã«ã‹ã‹ã£ãŸæ™‚é–“ã‚’æ¸¬å®š"""
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.virtual_memory().used
            
            self.metrics[operation_name] = {
                'duration': end_time - start_time,
                'memory_delta': end_memory - start_memory
            }
    
    def start_monitoring(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚’é–‹å§‹"""
        self.monitoring = True
        
        def monitor():
            while self.monitoring:
                cpu_percent = psutil.cpu_percent()
                memory = psutil.virtual_memory()
                
                print(f"\rğŸ”„ CPU: {cpu_percent:5.1f}% | "
                      f"ãƒ¡ãƒ¢ãƒª: {memory.percent:5.1f}% | "
                      f"åˆ©ç”¨å¯èƒ½: {memory.available//1024**2:,} MB", 
                      end='', flush=True)
                
                time.sleep(1)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
    
    def stop_monitoring(self):
        """ç›£è¦–ã‚’åœæ­¢"""
        self.monitoring = False
        print()  # ï¼ˆæ”¹è¡Œï¼‰
    
    def print_summary(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼:")
        print("-" * 40)
        
        for operation, metrics in self.metrics.items():
            duration = metrics['duration']
            memory_mb = metrics['memory_delta'] / 1024 / 1024
            
            print(f"{operation:20s}: {duration:8.2f}s | "
                  f"{memory_mb:+8.1f}MB")

# ä½¿ç”¨ä¾‹
monitor = PerformanceMonitor()
monitor.start_monitoring()

with monitor.measure("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œè¨¼"):
    validate_database(db_path)

with monitor.measure("ãƒ‡ãƒ¼ã‚¿å¤‰æ›"):
    convert_data()

monitor.stop_monitoring()
monitor.print_summary()
```

## ğŸ“‹ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ğŸ”§ å¤‰æ›å‰ã®ãƒã‚§ãƒƒã‚¯
- [ ] Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ â‰¥ 3.8
- [ ] ã™ã¹ã¦ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒæ­£ã—ã„ã“ã¨
- [ ] Fenixãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ç ´æã—ã¦ã„ãªã„ã“ã¨
- [ ] ååˆ†ãªåˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª (æ¨å¥¨ 4GBä»¥ä¸Š)
- [ ] ååˆ†ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ (æ¨å¥¨ 1GBä»¥ä¸Š)
- [ ] å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚‹ã“ã¨

### ğŸ“Š å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒã‚§ãƒƒã‚¯
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒæˆåŠŸã—ã¦ã„ã‚‹ã“ã¨
- [ ] ã™ã¹ã¦ã®å¿…é ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨
- [ ] åº§æ¨™ãƒ‡ãƒ¼ã‚¿ãŒæœ‰åŠ¹ç¯„å›²å†…ã«ã‚ã‚‹ã“ã¨
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé©åˆ‡ãªç¯„å›²å†…ã«ã‚ã‚‹ã“ã¨
- [ ] æ¨©é™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ãªã„ã“ã¨
- [ ] ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨

### ğŸ“ å¤‰æ›å¾Œã®æ¤œè¨¼
- [ ] ã™ã¹ã¦ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨
- [ ] ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒé©åˆ‡ã§ã‚ã‚‹ã“ã¨ï¼ˆç©ºã§ãªã„ã€ã¾ãŸã¯ç•°å¸¸ã«å¤§ãããªã„ï¼‰
- [ ] JSONå½¢å¼ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨
- [ ] åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãŒæˆåŠŸã—ã¦ã„ã‚‹ã“ã¨
- [ ] ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹ã“ã¨
- [ ] ãƒ­ã‚°ã«é‡å¤§ãªã‚¨ãƒ©ãƒ¼ãŒãªã„ã“ã¨

## ğŸ†˜ ãƒ˜ãƒ«ãƒ—ã®å…¥æ‰‹

### è‡ªå·±è¨ºæ–­
1.  **è¨ºæ–­ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ**ï¼š
    ```python
    from tfdi_converter.diagnostics import run_full_diagnostics
    run_full_diagnostics()
    ```

2.  **è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèª**ï¼š
    ```bash
    tail -f converter.log
    grep -i error converter.log
    ```

3.  **ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª**ï¼š
    ```bash
    # Windows
    taskmgr
       
    # macOS
    activity monitor
       
    # Linux
    top
    htop
    ```

### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒãƒ¼ãƒˆ
-   **GitHub Issues**: ãƒã‚°ã‚„æŠ€è¡“çš„ãªå•é¡Œã‚’å ±å‘Š
-   **GitHub Discussions**: ä½¿ç”¨ä¸Šã®å•é¡Œã¨çµŒé¨“ã®å…±æœ‰
-   **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: å®Œå…¨ãªä½¿ç”¨ã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§

### å•é¡Œã‚’å ±å‘Šã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
-   **å®Œå…¨ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°**
-   **ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒæƒ…å ±**
-   **ã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³**
-   **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±**ï¼ˆã‚µã‚¤ã‚ºã€AIRACãªã©ï¼‰
-   **å†ç¾æ‰‹é †**
-   **é–¢é€£ã™ã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**

---

**æœªè§£æ±ºã®å•é¡Œã«é­é‡ã—ã¾ã—ãŸã‹ï¼Ÿ** 

[GitHub Issues](https://github.com/your-org/tfdi-converter/issues) ã§æ–°ã—ã„å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ç§ãŸã¡ãŒã§ãã‚‹ã ã‘æ—©ãè§£æ±ºã‚’æ”¯æ´ã—ã¾ã™ï¼ğŸšâœ¨